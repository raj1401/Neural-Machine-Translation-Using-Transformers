# Neural-Machine-Translation-using-Transformers
 A transparent PyTorch implementation of the Transformer architecture for neural machine translation

This project implements the Transformer architecture from scratch in PyTorch to train a German-to-English translation model. It follows an object-orientated approach written in Jupyter Notebook so that it can easily be used on platforms like Google Colab and Kaggle.
