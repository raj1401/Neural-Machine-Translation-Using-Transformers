{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation Using Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering Data and preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import vocab\n",
    "from torchtext.utils import download_from_url, extract_archive\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_base = 'https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/'\n",
    "train_urls = ('train.de.gz', 'train.en.gz')\n",
    "val_urls = ('val.de.gz', 'val.en.gz')\n",
    "test_urls = ('test_2016_flickr.de.gz', 'test_2016_flickr.en.gz')\n",
    "\n",
    "train_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in train_urls]\n",
    "val_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in val_urls]\n",
    "test_filepaths = [extract_archive(download_from_url(url_base + url))[0] for url in test_urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_tokenizer = get_tokenizer('spacy', language='de_core_news_sm')\n",
    "en_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(filepaths, tokenizer):\n",
    "  counter = Counter()\n",
    "  for filepath in filepaths:\n",
    "    with io.open(filepath, encoding=\"utf8\") as f:\n",
    "      for string_ in f:\n",
    "        counter.update(tokenizer(string_))\n",
    "  return vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "\n",
    "de_vocab = build_vocab([train_filepaths[0], val_filepaths[0], test_filepaths[0]], de_tokenizer)\n",
    "en_vocab = build_vocab([train_filepaths[1], val_filepaths[1], test_filepaths[1]], en_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(filepaths):\n",
    "  raw_de_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
    "  raw_en_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n",
    "  data = []\n",
    "  for (raw_de, raw_en) in zip(raw_de_iter, raw_en_iter):\n",
    "    de_tensor_ = torch.tensor([de_vocab[token] for token in de_tokenizer(raw_de)], dtype=torch.long)\n",
    "    en_tensor_ = torch.tensor([en_vocab[token] for token in en_tokenizer(raw_en)], dtype=torch.long)\n",
    "    data.append((de_tensor_, en_tensor_))\n",
    "  return data\n",
    "\n",
    "train_data = data_process(train_filepaths)\n",
    "val_data = data_process(val_filepaths)\n",
    "test_data = data_process(test_filepaths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "PAD_IDX = de_vocab['<pad>']\n",
    "BOS_IDX = de_vocab['<bos>']\n",
    "EOS_IDX = de_vocab['<eos>']\n",
    "\n",
    "def generate_batch(data_batch):\n",
    "  de_batch, en_batch = [], []\n",
    "  for (de_item, en_item) in data_batch:\n",
    "    de_batch.append(torch.cat([torch.tensor([BOS_IDX]), de_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "    en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "  de_batch = pad_sequence(de_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "  en_batch = pad_sequence(en_batch, padding_value=PAD_IDX, batch_first=True)\n",
    "  return de_batch, en_batch\n",
    "\n",
    "training_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch, drop_last=True)\n",
    "validation_dataloader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch, drop_last=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch, drop_last=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the available device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the device\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Various Components of a Transformer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Positional Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size:int, d_model:int) -> None:\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model, padding_idx=0)\n",
    "        self.pos_encoding = self.positional_encoding(length=2048, depth=d_model)\n",
    "    \n",
    "    \n",
    "    def forward(self, x:Tensor) -> Tensor:\n",
    "        length = x.size(dim=1)\n",
    "        x = self.embedding(x)\n",
    "        # Scaling\n",
    "        x *= torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))\n",
    "        x = x + torch.unsqueeze(self.pos_encoding, dim=0)[:, :length, :]\n",
    "        return x\n",
    "    \n",
    "\n",
    "    def positional_encoding(self, length:int, depth:int) -> Tensor:\n",
    "        depth = depth/2\n",
    "\n",
    "        positions = np.arange(length)[:, np.newaxis]\n",
    "        depths = np.arange(depth)[np.newaxis, :] / depth\n",
    "\n",
    "        angle_rates = 1 / (10000**depths)\n",
    "        angle_rads = positions * angle_rates\n",
    "\n",
    "        pos_encoding = np.concatenate([np.sin(angle_rads), np.cos(angle_rads)], axis=-1).astype(\"float32\")\n",
    "\n",
    "        return torch.from_numpy(pos_encoding)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Headed Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Attention Head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, dim_in:int, dim_k:int, dim_v:int) -> None:\n",
    "        super(AttentionHead, self).__init__()\n",
    "        self.q = nn.Linear(in_features=dim_in, out_features=dim_k)\n",
    "        self.k = nn.Linear(in_features=dim_in, out_features=dim_k)\n",
    "        self.v = nn.Linear(in_features=dim_in, out_features=dim_v)\n",
    "    \n",
    "\n",
    "    def forward(self, query:Tensor, key:Tensor, value:Tensor, is_causal:bool) -> Tensor:\n",
    "        return self.scaled_dot_product_attention(self.q(query), self.k(key), self.v(value), is_causal)  \n",
    "\n",
    "    \n",
    "    def scaled_dot_product_attention(self, query:Tensor, key:Tensor, value:Tensor, is_causal:bool) -> Tensor:\n",
    "        if is_causal:\n",
    "            mask_size = (query.size(0), query.size(1), query.size(1))\n",
    "            mask = self.get_causal_mask(mask_size)\n",
    "            unmasked_attn_weights = query.bmm(key.transpose(dim0=1, dim1=2))\n",
    "            attn_weights = torch.mul(unmasked_attn_weights, mask)\n",
    "            del mask\n",
    "        else:\n",
    "            attn_weights = query.bmm(key.transpose(dim0=1, dim1=2))\n",
    "\n",
    "        scale = query.size(-1) ** 0.5\n",
    "        softmax_vals = F.softmax(attn_weights / scale, dim=-1)        \n",
    "        return softmax_vals.bmm(value)\n",
    "    \n",
    "\n",
    "    def get_causal_mask(self, mask_size:tuple) -> Tensor:\n",
    "        # Creates a mask for causal self-attention\n",
    "        mask = np.ones(shape=mask_size)\n",
    "        for row in range(mask.shape[1]):\n",
    "            mask[:, row, row+1:] *= 0\n",
    "        return torch.as_tensor(mask, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Head Attention (with optional is_causal for causal self-attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, num_heads:int, dim_in:int, dim_k:int, dim_v:int, is_causal:bool = False) -> None:\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        self.is_causal = is_causal\n",
    "        self.heads = nn.ModuleList(\n",
    "            [AttentionHead(dim_in, dim_k, dim_v) for _ in range(num_heads)]\n",
    "        )\n",
    "        self.linear = nn.Linear(num_heads * dim_k, dim_in)\n",
    "    \n",
    "\n",
    "    def forward(self, query:Tensor, key:Tensor, value:Tensor) -> Tensor:\n",
    "        concat_layer = torch.cat([head(query, key, value, self.is_causal) for head in self.heads], dim=-1)\n",
    "        return self.linear(concat_layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Position-wise Feedforward Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Feedforward():\n",
    "    def __init__(self, d_model:int = 512, d_ff:int = 2048) -> None:\n",
    "        self.d_model = d_model\n",
    "        self.d_ff = d_ff\n",
    "    \n",
    "\n",
    "    def get_feedforward(self) -> nn.Module:\n",
    "        return nn.Sequential(\n",
    "            nn.Linear(self.d_model, self.d_ff),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.d_ff, self.d_model)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Residual Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Residual(nn.Module):\n",
    "    def __init__(self, sublayer:nn.Module, dimension:int, dropout:float = 0.1) -> None:\n",
    "        super(Residual, self).__init__()\n",
    "        self.sublayer = sublayer\n",
    "        self.norm = nn.LayerNorm(dimension)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "\n",
    "    def forward(self, *tensors:Tensor) -> Tensor:\n",
    "        # Assumes query tensor is provided first\n",
    "        return self.norm(tensors[0] + self.dropout(self.sublayer(*tensors)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Encoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, num_heads:int, d_model:int, d_ff:int, dropout:float = 0.1) -> None:\n",
    "        super(TransformerEncoderLayer, self).__init__()\n",
    "        dim_k = dim_v = max(d_model // num_heads, 1)\n",
    "\n",
    "        self.global_self_attention = Residual(\n",
    "            MultiHeadedAttention(num_heads=num_heads, dim_in=d_model, dim_k=dim_k, dim_v=dim_v),\n",
    "            dimension=d_model,\n",
    "            dropout=dropout\n",
    "        )    \n",
    "\n",
    "        self.feedforward = Residual(\n",
    "            Feedforward(d_model=d_model, d_ff=d_ff).get_feedforward(),\n",
    "            dimension=d_model,\n",
    "            dropout=dropout\n",
    "        )\n",
    "    \n",
    "\n",
    "    def forward(self, x:Tensor) -> Tensor:\n",
    "        x = self.global_self_attention(x, x, x)\n",
    "        x = self.feedforward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, num_layers:int, d_model:int, num_heads:int, \n",
    "                 d_ff:int, vocab_size:int, dropout:float = 0.1) -> None:\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.pos_embedding = PositionalEmbedding(vocab_size, d_model)\n",
    "\n",
    "        self.enc_layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerEncoderLayer(\n",
    "                    num_heads=num_heads,\n",
    "                    d_model=d_model,\n",
    "                    d_ff=d_ff,\n",
    "                    dropout=dropout\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "    \n",
    "\n",
    "    def forward(self, x:Tensor) -> Tensor:\n",
    "        # 'x' is token-IDs shape (batch_size, seq_length)\n",
    "        x = self.pos_embedding(x) # Shape (batch_size, seq_length, d_model)\n",
    "\n",
    "        # Add Dropout\n",
    "        x = self.dropout_layer(x)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x)\n",
    "        \n",
    "        return x # Shape (batch_size, seq_length, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decoder Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoderLayer(nn.Module):\n",
    "    def __init__(self, num_heads:int, d_model:int, d_ff:int, dropout:float = 0.1) -> None:\n",
    "        super(TransformerDecoderLayer, self).__init__()\n",
    "\n",
    "        dim_k = dim_v = max(d_model // num_heads, 1)\n",
    "\n",
    "        self.causal_self_attention = Residual(\n",
    "            MultiHeadedAttention(num_heads=num_heads, dim_in=d_model, dim_k=dim_k, dim_v=dim_v, is_causal=True),\n",
    "            dimension=d_model,\n",
    "            dropout=dropout\n",
    "        )  \n",
    "\n",
    "        self.cross_attention = Residual(\n",
    "            MultiHeadedAttention(num_heads=num_heads, dim_in=d_model, dim_k=dim_k, dim_v=dim_v),\n",
    "            dimension=d_model,\n",
    "            dropout=dropout\n",
    "        )  \n",
    "\n",
    "        self.feedforward = Residual(\n",
    "            Feedforward(d_model=d_model, d_ff=d_ff).get_feedforward(),\n",
    "            dimension=d_model,\n",
    "            dropout=dropout\n",
    "        )\n",
    "    \n",
    "\n",
    "    def forward(self, x:Tensor, context:Tensor) -> Tensor:\n",
    "        x = self.causal_self_attention(x, x, x)\n",
    "        x = self.cross_attention(x, context, context)\n",
    "        x = self.feedforward(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, num_layers:int, d_model:int, num_heads:int,\n",
    "                 d_ff:int, vocab_size:int, dropout:float = 0.1) -> None:\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.pos_embedding = PositionalEmbedding(vocab_size, d_model)\n",
    "\n",
    "        self.dec_layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerDecoderLayer(\n",
    "                    num_heads=num_heads,\n",
    "                    d_model=d_model,\n",
    "                    d_ff=d_ff,\n",
    "                    dropout=dropout\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "    \n",
    "\n",
    "    def forward(self, x:Tensor, context:Tensor) -> Tensor:\n",
    "        # 'x' is token-IDs shape (batch_size, seq_length)\n",
    "        x = self.pos_embedding(x) # Shape (batch_size, seq_length, d_model)\n",
    "        \n",
    "        # Add Dropout\n",
    "        x = self.dropout_layer(x)\n",
    "        \n",
    "        for i in range(self.num_layers):\n",
    "            x = self.dec_layers[i](x, context)\n",
    "        \n",
    "        return x # Shape (batch_size, seq_length, d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, num_layers:int, d_model:int, num_heads:int, d_ff:int,\n",
    "                 input_vocab_size:int, target_vocab_size:int, dropout:float = 0.1) -> None:\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = TransformerEncoder(\n",
    "            num_layers=num_layers, d_model=d_model, num_heads=num_heads,\n",
    "            d_ff=d_ff, vocab_size=input_vocab_size, dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.decoder = TransformerDecoder(\n",
    "            num_layers=num_layers, d_model=d_model, num_heads=num_heads,\n",
    "            d_ff=d_ff, vocab_size=target_vocab_size, dropout=dropout\n",
    "        )\n",
    "\n",
    "        self.final_layer = nn.Linear(d_model, target_vocab_size)\n",
    "    \n",
    "\n",
    "    def forward(self, x:Tensor, context:Tensor) -> Tensor:\n",
    "        context = self.encoder(context)  # (batch_size, context_len, d_model)        \n",
    "        x = self.decoder(x, context)     # (batch_size, target_len, d_model)\n",
    "        logits = self.final_layer(x)     # (batch_size, target_len, target_vocab_size)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original Paper\n",
    "num_layers = 6\n",
    "d_model = 512\n",
    "d_ff = 2048\n",
    "num_heads = 8\n",
    "dropout = 0.1\n",
    "\n",
    "# # Reduced\n",
    "# num_layers = 4\n",
    "# d_model = 128\n",
    "# d_ff = 512\n",
    "# num_heads = 8\n",
    "# dropout = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Instantiation and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    num_layers=num_layers,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    d_ff=d_ff,\n",
    "    dropout=dropout,\n",
    "    input_vocab_size=len(de_vocab),\n",
    "    target_vocab_size=len(en_vocab)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate Scheduler Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warmup_steps = 4000\n",
    "\n",
    "def get_learning_rate(current_epoch:int) -> float:\n",
    "    if current_epoch == 0:\n",
    "        return 0\n",
    "    return (d_model ** -0.5) * min(current_epoch**-0.5, current_epoch * (warmup_steps ** -1.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "initial_learning_rate = 1   # Gets multiplied by LambdaLR Scheduler\n",
    "\n",
    "optimizer = torch.optim.Adam(transformer.parameters(), lr=initial_learning_rate, betas=(0.9, 0.98), eps=1e-9)\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=get_learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def masked_loss(pred:Tensor, label:Tensor):\n",
    "    mask = label != 0\n",
    "    loss_object = nn.CrossEntropyLoss(reduction=\"none\")\n",
    "    loss = loss_object(label, pred)\n",
    "\n",
    "    mask = mask.to(dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "\n",
    "    loss = torch.sum(loss) / torch.sum(mask)\n",
    "    return loss\n",
    "\n",
    "\n",
    "def masked_accuracy(pred:Tensor, label:Tensor):\n",
    "    pred = torch.argmax(pred, dim=2)\n",
    "    label = label.to(dtype=pred.dtype)\n",
    "    matching = label == pred\n",
    "\n",
    "    mask = label != 0\n",
    "\n",
    "    matching = torch.logical_and(matching, mask)\n",
    "\n",
    "    matching = matching.to(dtype=torch.float32)\n",
    "    mask = mask.to(dtype=torch.float32)\n",
    "\n",
    "    return torch.sum(matching) / torch.sum(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader:DataLoader, model:Transformer, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "\n",
    "    for batch, (context,x) in enumerate(dataloader):\n",
    "        context, x = context.to(device), x.to(device)\n",
    "        \n",
    "        # Computing prediction error\n",
    "        pred = model(x, context)\n",
    "        loss = loss_fn(pred.transpose(dim0=1, dim1=2), x)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if batch % 10 == 0:\n",
    "            loss, current = loss.item(), (batch+1) * len(context)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\", end=\"\\r\")\n",
    "    \n",
    "    print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "\n",
    "def validate(dataloader, model, loss_fn, accuracy_metric):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for context, x in dataloader:\n",
    "            context, x = context.to(device), x.to(device)\n",
    "            pred = model(x, context)\n",
    "            test_loss += loss_fn(pred.transpose(dim0=1, dim1=2), x).item()\n",
    "            correct += accuracy_metric(pred, x).item()\n",
    "            # correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "    return test_loss, correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_parameters = sum(p.numel() for p in transformer.parameters())\n",
    "print(f\"Total number of parameters = {num_parameters}\")\n",
    "\n",
    "PAD_IDX = en_vocab.get_stoi()['<pad>']\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "curr_loss, curr_acc = 1_000_000_000, 0\n",
    "save_on_loss = True   # if True, saves model when loss reduces, otherwise saves model when accuracy increases\n",
    "weights_path = \"transformer_weights.pth\"\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1} \\n ----------------------------\")\n",
    "    train(training_dataloader, transformer, loss_fn, optimizer)\n",
    "    loss, acc = validate(validation_dataloader, transformer, loss_fn, masked_accuracy)\n",
    "\n",
    "    # Model Save Checkpoints\n",
    "    if save_on_loss:\n",
    "        if (loss < curr_loss):\n",
    "            curr_loss = loss\n",
    "            curr_acc = acc\n",
    "            torch.save(transformer.state_dict(), weights_path)\n",
    "    else:\n",
    "        if (acc > curr_acc):\n",
    "            curr_acc = acc\n",
    "            curr_loss = loss\n",
    "            torch.save(transformer.state_dict(), weights_path)\n",
    "\n",
    "    scheduler.step()\n",
    "\n",
    "print(\"-------------Training Complete------------------\")\n",
    "print(f\"Saved Model's Loss = {curr_loss}\")\n",
    "print(f\"Saved Model's Accuracy = {100*curr_acc}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Translator():\n",
    "    def __init__(self, model:nn.Module, model_weights, input_language_tokenizer, output_language_tokenizer, \n",
    "                 input_language_vocab, output_language_vocab, max_tokens=200) -> None:\n",
    "        self.model = model\n",
    "        model.load_state_dict(torch.load(model_weights, map_location=device))\n",
    "        self.input_language_tokenizer = input_language_tokenizer\n",
    "        self.output_language_tokenizer = output_language_tokenizer\n",
    "        self.input_language_vocab = input_language_vocab\n",
    "        self.output_language_vocab = output_language_vocab\n",
    "        self.max_tokens = max_tokens\n",
    "        \n",
    "\n",
    "    def translate(self, input_sentence:str) -> str:\n",
    "        print(f\"Input Sentence: {input_sentence}\")\n",
    "        token_list = self.input_language_tokenizer(input_sentence)\n",
    "        bos_val = self.output_language_vocab.get_stoi()['<bos>']\n",
    "\n",
    "        output_sentence = \"\"\n",
    "        output_token_id_list = [bos_val]\n",
    "        last_predicted = bos_val\n",
    "\n",
    "        eos_val = self.output_language_vocab.get_stoi()['<eos>']\n",
    "\n",
    "        context_tensor = torch.tensor([self.input_language_vocab[token] for token in token_list], dtype=torch.long).unsqueeze(0)\n",
    "        print(context_tensor)\n",
    "\n",
    "        for i in range(self.max_tokens):\n",
    "            x_tensor = torch.tensor([token_id for token_id in output_token_id_list], dtype=torch.long).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                predicted_val = self.model(x_tensor, context_tensor)\n",
    "\n",
    "            last_predicted = predicted_val[0,-1,:].argmax()\n",
    "            del x_tensor\n",
    "            \n",
    "            if last_predicted == eos_val:\n",
    "                break\n",
    "            else:\n",
    "                output_token_id_list.append(last_predicted)\n",
    "                output_sentence += \" \" + self.output_language_vocab.lookup_token(last_predicted)\n",
    "\n",
    "        return output_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translator = Translator(model=transformer, model_weights=weights_path, \n",
    "                        input_language_tokenizer=de_tokenizer, output_language_tokenizer=en_tokenizer, \n",
    "                        input_language_vocab=de_vocab, output_language_vocab=en_vocab)\n",
    "\n",
    "with io.open(test_filepaths[0], encoding=\"utf8\") as f:\n",
    "    for string_ in f:\n",
    "        translated_sentence = translator.translate(string_)\n",
    "        print(translated_sentence)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
